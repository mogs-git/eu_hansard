---
title: 'Brexit: A Textual Analysis II'
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---


```{r include = FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

```{r, echo=F}
pacman::p_load(tidyverse, magrittr, tidytext) # igraph, ggraph, tidygraph, grid, gridExtra, ggpubr, networkD3)

full_speeches <- readRDS("..//data//full_speeches_2.RDAT")
appearances_tib <- readRDS("..//data//appearances_2.RDAT")
small_speeches <- readRDS("..//data//small_speeches.RDAT")

full_speeches$all_speeches %<>% map(~str_replace_all(., "\\\r\\\n\\\r\\\n", " "))
appearances_tib$speeches %<>% map(~str_replace_all(., "\\\r\\\n\\\r\\\n", " "))

appearances_tib %<>% mutate(speech_id = row_number())

appearances_tib %<>% mutate(speeches = map_chr(speeches, as.vector))

```

In my previous article[link], I explored how by looking at simple numerical features of text from a Hansard (number of speeches, counts of different words) and applying text mining methods like calculating the TF-IDF can help shape our perspective when analysing one of these documents. Taking inspiration again from Julia Silge and David Robinson's brilliant book on (tidy text mining)[https://www.tidytextmining.com/], this time we will explore how more advanced analytical methods and more creative ways of extracting information from the text can help us find and understand the important details of a Hansard. 

Last time, in order to analyse the text, we counted individuals words that are associated with categorical variables like the speaker, political party or speech number. But why just split the text up into individual words? We know that some words naturally appear in groups (Mr. Smith, United Kingdom, European Union), so we might want to count groups of words- collectively known as "n-grams". We are specifically going to count pairs of words, which are known to text-mining aficionados as "bigrams" (which I have a habit of reading as *big rams*, making big rams a big ram).  

This time around, I'm going to include some R code to help tell the story.

```{r}
appearances_tib %>%
  mutate(speeches = str_sub(speeches, 1, 100)) %>% # Shorten speeches for viewer's pleasure
  head()  
```

In case you've forgotten from last time, here is our main data table for the analysis scraped from the Hansard plaintext by yours truly. We've got columns for the speaker, party, gender, speech, party again (as a factor for plotting, and with "small" parties condensed into "other") and the speech_id, which just describes the sequence speeches happened in. 

Now let's translate the words in those speeches into bigrams:
```{r echo=TRUE}
# Each row corresponds to a bigram from each speech now.
bigrams_tibble <- appearances_tib %>% 
  unnest_tokens(bigram, speeches, token = "ngrams", n = 2) 

bigrams_tibble %>%
  select(name, speech_id, bigram) %>%
  head()
```

```{r echo=TRUE}
# For our analysis, we don't want bigrams like "I am".
# We're going to remove all bigrams that contain a stop word (see glossary).

# put words from each bigram into separate columns
bigrams_separated <- bigrams_tibble %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# filter out rows where either word from the bigram is a stop word
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# we can then put both words back into the same column if we feel like it
# bigrams_united <- bigrams_filtered %>%
#   unite(bigram, word1, word2, sep = " ")

# For now, we will count each pair of words
(bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE))
```

These are just the raw counts of bigrams so you get the idea. 

Now let's visualise the bigrams. We drop every pair of words that appears less than five times in the whole Hansard. Then put each remaining word on a graph, and connect lines between the ones that occur together in pairs. The strength of the line tells us how often the pair of words were used. An algorithm sorts out the points so that connected nets of words are plotted close to one another and not overlapping with other nets. 

```{r, fig.width=10, fig.height=10}
library(igraph)
bigram_graph <- bigram_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame(set.seed(2019))

library(ggraph)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = log(n)), show.legend=F) +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  scale_edge_alpha(range = c(0.5,1)) +
  theme_void()
```


There is a lot of cool stuff going on in this graph. We can see how by pairing up words, they arrange themselves into families of topics. There are personal declarative words used to describe other MPs. There are buzzwords/phrases (cliff edge, bargaining chips). There are words related to the underlying framework of Hansard debates (amendments and clauses/subsections). There are different words associated with different topics (industries, human rights). 

From glancing at this graph, we can learn immediately the amendments discussed, industries that got mentioned and other issues that were discussed.  

This might help guide further exploration: Here are some examples of what we might now look for,

### Which Lords mentioned other Lords most often

```{r}
appearances_tib %>% 
  mutate(speeches = map_chr(speeches, as.vector)) %>%
  unnest_tokens(bigram, speeches, token = "ngrams", n = 2, to_lower = F) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(word1 %in% c("Lord","Baroness","Lady", "Viscount", "Earl"),
         !word2 %in% c("Lord", "Lady")) %>%
  unite(bigram, c(word1, word2), sep=" ") %>%
  count(name, bigram) %>%
  filter(str_detect(bigram, "[A-Za-z] [A-Z][a-z]*")) %>%
  group_by(name) %>% summarise(total_times_others_mentioned = sum(n)) %>% arrange(desc(total_times_others_mentioned))
```

### Getting context on amendments

Perhaps we want to see more about how people speak about amendments. We can create a function which picks out sentences where our word of interest was used.  
```{r}
wordInContext_sentence <- function(txt, word) {
  # lower case full string vector (txt) and search term (word).
  txt %>% str_extract_all(str_c("([^\\.\\?\\!]*\\s)", word, "(\\s|,|:|;)[^\\.\\?\\!]*"))
}

# wordInContext_sentence(str_to_lower(appearances_tib$speeches), "amendment")
appearances_tib %>%
  mutate(sents_of_interest = map(speeches, ~unlist(wordInContext_sentence(str_to_lower(.), "amendment")))) %>%
  select(name, speech_id, sents_of_interest) %>%
  unnest() 
```

Now let's dig into this in more detail...
```{r}
amendment_tib <- appearances_tib %>%
  mutate(amendment = str_extract(str_to_lower(speeches), "amendment [0-9][a-z0-9]+\\W"), 
         amendment = str_sub(amendment, end = -2)) %>% 
  select(-speeches) %>%
  mutate(explicit_mention = !is.na(amendment)) # mark speeches that mentioned amendments for later

amendment_tib
```
Here we just have the same data as before, but with a column that tells us which amendments were mentioned in that speech. (Keen readers might have noticed I used str_extract() rather than str_extract_all(), for the time being we will just extract the first amendment mentioned in each speech to simplify later analysis). 

Now we are going to do something cheeky. Not every speech mentions an amendment. But maybe we can assume that the speeches that follow the previously mentioned amendment are also all referring to that one. We can "populate" the amendment column by replacing NAs (missing values) with the previous amendment mentioned. I'll call these "implicit mentions".

```{r}
for (i in seq_along(amendment_tib$amendment)) {
  if (!is.na(amendment_tib$amendment[i])) {
    current <- amendment_tib$amendment[i]
  }
  if (is.na(amendment_tib$amendment[i])) {
    amendment_tib$amendment[i] <- current
  }
}
```

We can now do a quick sanity check to make sure that the number of implicit mentions of an amendment don't stray too far from what we would expect based on the number of explicit mentions.

```{r message=FALSE, error=F, warning=F}
# count explicit and implicit mentions of each amendment

explicit_mentions <- amendment_tib %>%
  filter(explicit_mention) %>%
  count(amendment)

implicit_mentions <- amendment_tib %>%
  count(amendment)

# sanity check
implicit_mentions %>%
  left_join(rename(explicit_mentions, n_explicit = n)) %>%
  ggplot(aes(n_explicit, n)) + geom_point()
```

The numbers appear to be correlated, let's continue using the implicit mentions so we can access more data. 
What can we do with this data? One thing I'm interested in is if peers from different parties talked about each amendment different amounts. We could just plot the raw number of times each amendment was mentioned by each party, but because we know speakers from some parties spoke more than others we will instead use the proportion of speeches made about each amendment by each party. (I also removed amendments that were mentioned in less than five speeches, and only look at the groups that gave a good number of speeches).

```{r}
amendment_tib %>%
  count(party, amendment) %>%
  group_by(amendment) %>%
  mutate(total_mentions = sum(n)) %>%
  filter(total_mentions > 5) %>%
  group_by(party) %>%
  mutate(tot = sum(n), p = n/tot) %>%
  filter(party %in% c("(CB)", "(Con)", "(Lab)", "(LD)")) %>%
  ggplot(aes(party, p)) + geom_col(aes(fill = amendment)) + scale_fill_brewer(type = "qual", palette = 2)

```

This is a pretty crude analysis, but is a nice summary of the importance of each amendment as a whole and to each party. One thing we can't see here is whether speeches supported the amendment or spoke against it. 
I had a go at this, again using simple, maybe crude but otherwise transparent methods strung together. This involved pulling out sentences that included the word "amendment" to remove noise, then using sentiment analysis as a proxy for support or opposition. I wasn't too happy with the result. Maybe this kind of classification is more appropriate for a machine learning approach. My code is displayed below:

```{r message=F, warning=F, error=F}
# Use a home-made function "words in context", to extract all the sentences from a speech containing a particular word. 
targeted_sent <- appearances_tib %>%
  mutate(sents_of_interest = map(speeches, ~unlist(wordInContext_sentence(str_to_lower(.), "amendment")))) %>%
  select(name, party, speech_id, sents_of_interest) %>% 
  unnest() %>%
  mutate(sent_id = row_number())

# Tokenise the sentences containing the word "amendment" for further analysis
targeted_sent %<>% 
  unnest_tokens(word, sents_of_interest)

# Using a sentiment library, score each sentence.
bing <- get_sentiments("bing")

# Count the number of positive and negative words in each "amendment sentence"
amendment_counts <- targeted_sent %>%
    filter(!word %in% my_stop_words) %>%
    inner_join(bing) %>%
    group_by(speech_id) %>%
    count(sentiment) 

# Take those counts and classify the position of each sentence on the amendment.
amendment_position <- amendment_counts %>%
  spread(sentiment, n) %>% 
  mutate(negative = ifelse(is.na(negative), 0, negative),
         positive = ifelse(is.na(positive), 0, positive),
         position = ifelse(positive>negative, "support",
                           ifelse(negative>positive, "oppose", "neutral")))

amendment_id <- amendment_tib %>% distinct(speech_id, amendment)

amendment_position %>%
  left_join(amendment_id) %>%
  filter(amendment %in% c("amendment 9b", "amendment 21", "amendment 25", "amendment 39", "amendment 17")) %>%
  count(amendment, position) %>%
  ggplot(aes(position, n)) + geom_col() + facet_wrap(~amendment)

```

Why am I not happy with this result?

Every step we did has a hole in it, and these problems compound until our results aren't really valid.

- Sentences are short- sometimes there aren't many "sentiment words" in them.
- The sentiment words available might not reflect the position of the sentence.
- The sentiment library (we used bing) isn't perfect.
- I didn't bother to take into account negation of words.
- Is a sentence with 4 positive words and 3 negative really in support of an amendment?
- The assumption that positive/negative is a good proxy for support/oppose.

Below are illustrative examples...

Of a true positive (A genuine opposition we recorded as oppositon):

```{r message=F, warning=F, error=F}
left_join(appearances_tib[appearances_tib$speech_id==184,],
          amendment_position[amendment_position$speech_id==184,],by = "speech_id") %>%
  unnest() %>%
  select(name, negative, positive, position, speeches)
```

And a false positive (A "support" we recorded as opposition):

```{r}
left_join(appearances_tib[appearances_tib$speech_id==177,],
          amendment_position[amendment_position$speech_id==177,],by = "speech_id") %>% 
  unnest() %>%
  select(name, negative, positive, position, speeches)
```


Ultimately, we can just look to see how peers voted (for example, on [amendment 9b](https://www.parliament.uk/business/publications/business-papers/lords/lords-divisions/?fd=2017-03-01&td=2017-03-03&dd=2017-03-01&division=1) or by reading the [research briefings](https://researchbriefings.parliament.uk/ResearchBriefing/Summary/CBP-7922)) 


### Topic modelling

Topic modelling, the basics of which is well explained in [tidy text mining for R](https://www.tidytextmining.com/topicmodeling.html), takes some documents and builds topics from the words in them, before assigning the probability that each document belongs in each topic. You have to input the number of topics that will be generated from your topics. Latent Dirichlet Allocation is the "generative process" that assigns words to topics, and topics to documents. 

Now there's a vague description if there ever was one. This is a technique I was hesitant to use, having not had time to fully understand the maths behind LDA. But I've decided to ignore my fears of black boxes for now, and comfort myself with the fact that this whole thing I'm writing is only a proof of concept. One day I'll get around to writing a full explanation of how LDA works for the layman.

We're going to use topic modelling to condense information from our speeches down into a selection of topics, not unlike what I tried to do using TF-IDF in a previous article. I've picked a beautifully arbitray 4 topics (actually, not completely. This is based on observing the types of words that came out of our TF-IDF analysis in the previous article).

```{r}
library(topicmodels)

my_stop_words <- c("lord", "lords", "noble", "baroness", "lordship", "minister", "amendment", "parliament", "eu", "uk", "government", "european", "union", "united", "kingdom")

at_words <- appearances_tib %>%
  anti_join(small_speeches) %>%
  unnest_tokens(word, speeches) %>%
  anti_join(stop_words) %>%
  filter(!word %in% my_stop_words, !str_detect(word, "[0-9]")) %>%
  count(speech_id, word, sort = TRUE) %>%
  ungroup() 

speeches_dtm <- at_words %>%
  cast_dtm(speech_id, word, n)

speeches_dtm

speeches_lda <- LDA(speeches_dtm, k = 4, control = list(seed = 1234))

speeches_lda

speech_topics <- tidy(speeches_lda, matrix = "beta")

speech_topics

top_terms <- speech_topics %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Above are our topics, and which 8 words are associated with each topic with the highest probability. I'd say we have a topics for "citizens/nationals", "nuclear policy/euratom", "womens/workers rights", and "devolved administrations". 

Now let's look at the flipside. How well were topics assigned to each document?

For each document, every topic is given a "gamma" value. This represents the probability that this topic describes this document. So in our case, each speech has four gamma values associated with it, one for each topic. What does the distribution of these probabilities look like?

```{r error=F, warning=F}
speeches_gamma %>%  
  group_by(speech_id) %>%
  filter(gamma == max(gamma)) %>%
  ggplot(aes(gamma)) + geom_histogram(bins = 30)

```

What does this histogram mean? The vast majority of topic assignments had a probability close to 1, meaning that most documents could be classified solidly as one of our topics. But what about those that weren't?

```{r}
sketchy_speeches <- speeches_gamma %>% 
  group_by(speech_id) %>%
  filter(gamma == max(gamma)) %>%
  filter(gamma < 0.95) %>%
  ungroup() %>%
  mutate(speech_id = as.numeric(speech_id))

speeches_gamma %>%
  filter(speech_id %in% sketchy_speeches$speech_id) %>%
  ggplot(aes(topic, gamma)) + geom_col(aes(fill = as.factor(topic))) + facet_wrap(~speech_id)
```

Just from eyeballing this, we can see something pretty cool- none of our sketchy speeches seem to be shared between "green and blue" or topic 2 and 3. These are the topics for euratom and rights.

I've counted up how often two topics are "difficult to assign" below:
```{r echo=F}
argh <- speeches_gamma %>%
  filter(speech_id %in% sketchy_speeches$speech_id) %>% group_by(speech_id) %>% top_n(2) %>% arrange(desc(speech_id)) %>% group_by(speech_id) %>% nest() %>% mutate(topics = map(data, ~c(pull(., topic)))) %>%
  select(-data) 

tibble(t1 = unlist(map(argh$topics, `[`(1))), t2 = unlist(map(argh$topics, `[`(2)))) %>%
  count(t1, t2)
```
Our topic on devolved administrations is often confused with every other topic, and the same is true for the topic on citizens rights. 

### Which topics where most important to each party?

```{r echo=F, warning=F, message=F}
party_id <- distinct(appearances_tib, speech_id, party)

speeches_gamma %>%  
  group_by(speech_id) %>%
  filter(gamma == max(gamma)) %>%
  ungroup() %>%
  mutate(speech_id = as.numeric(speech_id)) %>%
  left_join(party_id, by= "speech_id") %>%
  count(topic, party) %>%
  group_by(party) %>%
  mutate(n_party = sum(n), p = n/n_party) %>%
  filter(party %in% c("(CB)", "(Con)", "(Lab)", "(LD)")) %>%
  ggplot(aes(topic, p)) + geom_col() + facet_wrap(~party)
```

#### A "topic timeline"

```{r}
speeches_gamma %>%
  ggplot(aes(as.numeric(speech_id), gamma, colour = as.factor(topic))) +
  geom_point() +
  geom_smooth() +
  theme_bw()
```

#### As a final aside



#### Expanding our horizons

We are beginning to see how simply tools for analysis might reveal new things, if we use them creatively. One feature of the text particularly important to a debate or discussion is the use of questions. The questions asked in a Hansard tell a story. Rhetorical questions often highlight or emphasise salient points of an argument. Others are used to test another Lords argument. Some are sincere searches for answers. Others are used to probe an issue and give pause for thought, as put by Baroness Hamwee "I leave these questions hanging." (Column 837). 

Firstly, we can count the number of questions asked by each Lord, and in regard to certain topics. We can a

NEW: Maybe we can calculate number of nouns/adjs/verbs used etc... then cluster documents based on these values or euclidian distance + heirarchical clustering. 


```{r}


```

#### 

```{r}

```
### Discussion

There's something to be said here for the general role of exploratory data analysis in political and social discourse and decision making. Sometimes, analyses can give the impression that a researcher didn't think to themselves "just because we can, doesn't mean that we should". Every tool is only as useful as the practitioner is skillful. A more abstract method used in the wrong situation only obscures. Obviously here, I have been trying to showcase how different tools might be used in the context of exploring a parliamentary debate through the lens of a data analysis problem without being too interested in the results. But I hope I have also communicated to some extent the challenges inherent in this process- particularly at the ends of meaningful implementation and interpretation. 

Such tools should one day form a core part of a political transparency and accountability. The [house of commons library](https://commonslibrary.parliament.uk/about-us/) publishes research by independent experts who also provide impartial briefings to MPs on issues. This is really great. But I think it is only one part of transparency required for a proper functioning democracy. There is no substitute from giving people the power to dig into the data themselves, and often in such analysis the end product is nice but how one got there is obscured. This [Yougov piece](https://yougov.co.uk/topics/politics/articles-reports/2019/01/29/16-groups-brexit) is a great example of what I mean. Super interesting ideas- but how did they get there? What were the limitations of their approach? Let us see the guts and grime, and reproduce the analysis for ourselves*. As such, open source tools like those built in R I think really have the power to change how people participate in politics in a more intimate sense. You cannot get closer to the reality of parliamentary opinion than the Hansards themselves. Tools to navigate and analyse them in more detail, by more different people 

* Note that if you dig through far enough you'll see that 1754 people were sampled.
